2023-05-03 12:31:27,649 INFO gridtk: Starting job 96: /idiap/temp/esarkar/miniconda/envs/pytorch_vad/bin/python /idiap/project/evolang/meerkats_imen/evolang_meerkats_calls_classification/workspace/pretrainMara-marta.py -dir /idiap/project/evolang/meerkats_imen/dataset/Meerkat_sound_files_examples_segments/info_file.csv -s 16000 -b 16 -lr 0.003
/idiap/temp/esarkar/miniconda/envs/pytorch_vad/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
wandb: Currently logged in as: imenbm. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in ./wandb/run-20230503_123204-3n2xs7zk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretrained-mara-on-marta-notfixed-weight
wandb: â­ï¸ View project at https://wandb.ai/imenbm/marta_meerkat
wandb: ğŸš€ View run at https://wandb.ai/imenbm/marta_meerkat/runs/3n2xs7zk
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type     | Params
-----------------------------------
0 | model | PalazCNN | 23.3 K
-----------------------------------
23.3 K    Trainable params
0         Non-trainable params
23.3 K    Total params
0.093     Total estimated model params size (MB)
/idiap/temp/esarkar/miniconda/envs/pytorch_vad/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (72) is smaller than the logging interval Trainer(log_every_n_steps=75). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
`Trainer.fit` stopped: `max_epochs=100` reached.
Restoring states from the checkpoint path at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from checkpoint at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200.ckpt
Restoring states from the checkpoint path at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from checkpoint at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/idiap/temp/esarkar/miniconda/envs/pytorch_vad/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory ./marta_meerkat/3n2xs7zk/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type     | Params
-----------------------------------
0 | model | PalazCNN | 23.3 K
-----------------------------------
23.3 K    Trainable params
0         Non-trainable params
23.3 K    Total params
0.093     Total estimated model params size (MB)
/idiap/temp/esarkar/miniconda/envs/pytorch_vad/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (72) is smaller than the logging interval Trainer(log_every_n_steps=75). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
`Trainer.fit` stopped: `max_epochs=100` reached.
Restoring states from the checkpoint path at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v1.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from checkpoint at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v1.ckpt
Restoring states from the checkpoint path at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v1.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from checkpoint at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v1.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/idiap/temp/esarkar/miniconda/envs/pytorch_vad/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory ./marta_meerkat/3n2xs7zk/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type     | Params
-----------------------------------
0 | model | PalazCNN | 23.3 K
-----------------------------------
23.3 K    Trainable params
0         Non-trainable params
23.3 K    Total params
0.093     Total estimated model params size (MB)
/idiap/temp/esarkar/miniconda/envs/pytorch_vad/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (72) is smaller than the logging interval Trainer(log_every_n_steps=75). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
`Trainer.fit` stopped: `max_epochs=100` reached.
Restoring states from the checkpoint path at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v2.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from checkpoint at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v2.ckpt
Restoring states from the checkpoint path at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v2.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from checkpoint at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v2.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/idiap/temp/esarkar/miniconda/envs/pytorch_vad/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory ./marta_meerkat/3n2xs7zk/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type     | Params
-----------------------------------
0 | model | PalazCNN | 23.3 K
-----------------------------------
23.3 K    Trainable params
0         Non-trainable params
23.3 K    Total params
0.093     Total estimated model params size (MB)
/idiap/temp/esarkar/miniconda/envs/pytorch_vad/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (72) is smaller than the logging interval Trainer(log_every_n_steps=75). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
`Trainer.fit` stopped: `max_epochs=100` reached.
Restoring states from the checkpoint path at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v3.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from checkpoint at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v3.ckpt
Restoring states from the checkpoint path at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v3.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from checkpoint at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v3.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/idiap/temp/esarkar/miniconda/envs/pytorch_vad/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory ./marta_meerkat/3n2xs7zk/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type     | Params
-----------------------------------
0 | model | PalazCNN | 23.3 K
-----------------------------------
23.3 K    Trainable params
0         Non-trainable params
23.3 K    Total params
0.093     Total estimated model params size (MB)
/idiap/temp/esarkar/miniconda/envs/pytorch_vad/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (72) is smaller than the logging interval Trainer(log_every_n_steps=75). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
`Trainer.fit` stopped: `max_epochs=100` reached.
Restoring states from the checkpoint path at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v4.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from checkpoint at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v4.ckpt
Restoring states from the checkpoint path at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v4.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from checkpoint at ./marta_meerkat/3n2xs7zk/checkpoints/epoch=99-step=7200-v4.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch â–â–‚â–ƒâ–„â–…â–†â–‡â–‡â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ
wandb:      test_acc_epoch â–‚â–‡â–ƒâ–ƒâ–‚â–ˆâ–‚â–ˆâ–â–ˆ
wandb:       test_acc_step â–‡â–‚â–„â–„â–‡â–‡â–‡â–ˆâ–„â–…â–„â–‚â–„â–„â–‚â–‚â–…â–„â–„â–‡â–ˆâ–‡â–‡â–ˆâ–…â–‚â–‚â–…â–ˆâ–ˆâ–ˆâ–…â–‚â–‡â–‚â–â–ˆâ–ˆâ–ˆâ–ˆ
wandb:     test_loss_epoch â–…â–‚â–ƒâ–ƒâ–…â–â–„â–â–ˆâ–
wandb:      test_loss_step â–â–‚â–‚â–ˆâ–â–â–â–â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–
wandb:     train_acc_epoch â–â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:      train_acc_step â–â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–‚â–…â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–†â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    train_loss_epoch â–ˆâ–„â–‚â–â–‚â–â–â–‚â–ˆâ–„â–‚â–â–â–â–‚â–â–ˆâ–ƒâ–ƒâ–â–â–â–â–â–†â–ƒâ–‚â–â–â–â–â–â–†â–ƒâ–‚â–â–‚â–â–â–
wandb:     train_loss_step â–ˆâ–†â–‚â–â–â–â–â–„â–‡â–„â–‚â–‚â–‚â–â–â–â–…â–„â–‚â–â–â–â–â–â–„â–‚â–‚â–â–â–â–â–â–†â–ƒâ–‚â–â–â–â–‚â–
wandb: trainer/global_step â–â–â–‚â–‚â–†â–‚â–ƒâ–ƒâ–â–ƒâ–‚â–‚â–‚â–‚â–ˆâ–ƒâ–â–â–‚â–…â–‚â–‚â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–‡â–ƒâ–ƒâ–â–â–„â–‚â–‚â–ƒâ–ƒâ–
wandb: unbalanced accuracy â–„â–†â–„â–ƒâ–…â–†â–ƒâ–ˆâ–â–‡
wandb:       val_acc_epoch â–‚â–…â–†â–‡â–‡â–†â–‡â–‡â–â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–„â–†â–‡â–ˆâ–ˆâ–…â–‡â–†â–ƒâ–†â–‡â–†â–‡â–‡â–‡â–‡â–ƒâ–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆ
wandb:        val_acc_step â–…â–…â–…â–…â–ˆâ–‡â–…â–†â–…â–ˆâ–…â–…â–…â–…â–‡â–†â–â–†â–†â–†â–ˆâ–ˆâ–…â–…â–‡â–…â–‡â–„â–‡â–ˆâ–ˆâ–†â–…â–†â–…â–ˆâ–†â–…â–†â–…
wandb:      val_loss_epoch â–‚â–â–‚â–‚â–â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–‚â–‚â–ƒâ–ƒâ–„â–‚â–â–â–â–‚â–„â–ƒâ–ƒâ–‚â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–„â–†â–‡â–ˆâ–†â–ˆ
wandb:       val_loss_step â–‚â–‚â–ƒâ–‚â–â–‚â–…â–‚â–„â–â–ƒâ–ƒâ–ˆâ–‚â–„â–†â–„â–ƒâ–ƒâ–…â–â–â–ƒâ–…â–ƒâ–ƒâ–‚â–‡â–â–â–â–‚â–ƒâ–ƒâ–‡â–‚â–„â–…â–‚â–…
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:      test_acc_epoch 0.96936
wandb:       test_acc_step 1.0
wandb:     test_loss_epoch 0.15344
wandb:      test_loss_step 0.07092
wandb:     train_acc_epoch 1.0
wandb:      train_acc_step 1.0
wandb:    train_loss_epoch 0.00548
wandb:     train_loss_step 0.00316
wandb: trainer/global_step 0
wandb: unbalanced accuracy 0.8944
wandb:       val_acc_epoch 0.86063
wandb:        val_acc_step 0.86667
wandb:      val_loss_epoch 1.87721
wandb:       val_loss_step 0.65574
wandb: 
wandb: Synced pretrained-mara-on-marta-notfixed-weight: https://wandb.ai/imenbm/marta_meerkat/runs/3n2xs7zk
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230503_123204-3n2xs7zk/logs
2023-05-03 12:49:23,651 INFO gridtk: Job 96 finished with result 0
