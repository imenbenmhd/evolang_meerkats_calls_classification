{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/idiap/home/ibmahmoud/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import meerkats\n",
    "import config\n",
    "\n",
    "from meerkats import src\n",
    "from meerkats.src.models.Palazcnn import PalazCNN\n",
    "from meerkats.src.models.lit import Lit\n",
    "from meerkats.src.data.nccrmeerkatsdataset import NCCRMeerkatsDataset\n",
    "from meerkats.src.data.isabelmeerkatdataset import isabelMerkatDataset\n",
    "from meerkats.src.utils import utils\n",
    "from meerkats.src.features_extraction.extract_feats_segments import smile_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR=config.GITROOT + \"/experiments/\" #dir for checkpoints\n",
    "DATA_DIR=config.DATAMARA   # to change depend on data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract last layer of best cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1286 data points in the test set \n",
      "There are 5142 data points in the train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/idiap/home/ibmahmoud/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# lit=Lit(model=PalazCNN(n_input=1,n_output=6),num_classes=6,learning_rate=0.003)\n",
    "\n",
    "# checkpoint=torch.load(MAIN_DIR + \"experiment_/checkpoints/epoch=99-step=9600-v1.ckpt\",map_location=torch.device('cpu')) #best marta model\n",
    "# lit.load_state_dict(checkpoint[\"state_dict\"])\n",
    "# lit.model.eval()\n",
    "with open(config.GITROOT + \"/meerkats/src/data/class_to_index_mara.json\") as f:\n",
    "        class_to_index = json.load(f)\n",
    "data_test = isabelMerkatDataset(\n",
    "            audio_dir=DATA_DIR,\n",
    "            class_to_index=class_to_index,\n",
    "            target_sample_rate=16000,\n",
    "            train=False) \n",
    "   \n",
    "    \n",
    "data_train = isabelMerkatDataset(\n",
    "        audio_dir=DATA_DIR,\n",
    "        class_to_index=class_to_index,\n",
    "        target_sample_rate=16000,\n",
    "        train=True) \n",
    "result={}\n",
    "k_folds=5\n",
    "dataset=torch.utils.data.ConcatDataset([data_test,data_train])\n",
    "labels=dataset.datasets[0].filelist.class_index.tolist()+ dataset.datasets[1].filelist.class_index.tolist()\n",
    "kfold=StratifiedKFold(n_splits=k_folds,shuffle=True,random_state=42)\n",
    "\n",
    "accuracies_folds=[]\n",
    "\n",
    "for fold,(train_ids,test_ids) in enumerate(kfold.split(dataset,labels)):\n",
    "        if fold==1:\n",
    "                \n",
    "                num_train = len(train_ids)\n",
    "                split = int(np.floor(0.2* num_train))\n",
    "                                        #numpy.random.shuffle(indices)\n",
    "\n",
    "                                        # mask=dataset.filelist.index.isin(train_ids)\n",
    "                                        # dataset_=dataset.filelist[mask] \n",
    "                                        # all_ids=numpy.arange(0,len(dataset))\n",
    "                                        # mask=x.index.isin(train_ids)\n",
    "                                        # dataset_=x[mask]\n",
    "                                        # for k, (train_idx, valid_idx) in enumerate(kfold.split(dataset_,dataset_.class_index)):\n",
    "                                        #         train_ids=train_idx\n",
    "                                        #         break\n",
    "                                        # all_ids=numpy.arange(0,len(dataset))\n",
    "                                        # keep_mask=numpy.isin(all_ids,test_ids,invert=True)\n",
    "                                        # new_ids=all_ids[keep_mask]\n",
    "                                        # train_ids=new_ids[train_ids]\n",
    "                                        # valid_idx=new_ids[valid_idx]\n",
    "                print(f'There are {len(test_ids)} data points in the test set ')\n",
    "                print(f'There are {len(train_ids)} data points in the train set') \n",
    "                train_subsampler=torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "                test_subsampler=torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "                                # Dataloader in this fold\n",
    "                train_loader = DataLoader(dataset, batch_size=1,sampler=train_subsampler,collate_fn=utils.collate_fn, num_workers=4)\n",
    "                test_loader = DataLoader(dataset, batch_size=1,sampler=test_subsampler, shuffle=False,collate_fn=utils.collate_fn, num_workers=4)\n",
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (29062388.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[79], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    for batch in iter(test_loader):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "  FEATS=[] # list of all the features\n",
    "                for batch in iter(test_loader):\n",
    "                        input,labels=batch\n",
    "                # forward pass [with feature extraction]\n",
    "                        x,intermediate=lit.model(input)\n",
    "                        output=np.c_[intermediate.detach().numpy(),labels.detach().numpy()]\n",
    "                        FEATS.append(output)\n",
    "                        \n",
    "                FEATS_test=np.array(FEATS)\n",
    "                FEATS_test=FEATS_test.reshape((FEATS_test.shape[0]*FEATS_test.shape[1]), FEATS_test.shape[2]) # final features numpy\n",
    "                FEATS=[]\n",
    "                for batch in iter(train_loader):\n",
    "                        input,labels=batch\n",
    "                # forward pass [with feature extraction]\n",
    "                        x,intermediate=lit.model(input)\n",
    "                        output=np.c_[intermediate.detach().numpy(),labels.detach().numpy()]\n",
    "                        FEATS.append(output)\n",
    "                        \n",
    "                FEATS_train=np.array(FEATS)\n",
    "                print(FEATS_train[:,-1])\n",
    "                FEATS_train=FEATS_train.reshape((FEATS_train.shape[0]*FEATS_train.shape[1]), FEATS_train.shape[2]) # final features numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist=pd.concat((dataset.datasets[0].filelist,dataset.datasets[1].filelist))\n",
    "\n",
    "mask=filelist.index.isin(train_ids)\n",
    "dataset_train=filelist[mask] \n",
    "mask=filelist.index.isin(test_ids)\n",
    "dataset_test=filelist[mask]\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m smile\u001b[39m=\u001b[39msmile_features(dataset_train\u001b[39m.\u001b[39mpath,\u001b[39m\"\u001b[39m\u001b[39meGeMAPsv02\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(callable(smile))\n\u001b[0;32m----> 3\u001b[0m smile\u001b[39m.\u001b[39;49mextract()\n",
      "File \u001b[0;32m/remote/idiap.svm/project.evolang/meerkats_imen/evolang_meerkats_calls_classification/scripts/Notebooks/../../meerkats/src/features_extraction/extract_feats_segments.py:23\u001b[0m, in \u001b[0;36msmile_features.extract\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     22\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature\n\u001b[0;32m---> 23\u001b[0m     smile\u001b[39m=\u001b[39mopensmile\u001b[39m.\u001b[39mSmile(feature_set\u001b[39m=\u001b[39mopensmile\u001b[39m.\u001b[39;49mFeatureSet\u001b[39m.\u001b[39;49mname,feature_level\u001b[39m=\u001b[39mopensmile\u001b[39m.\u001b[39mFeatureLevel\u001b[39m.\u001b[39mFunctionals)\n\u001b[1;32m     24\u001b[0m     features\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mempty()\n\u001b[1;32m     25\u001b[0m     \u001b[39mfor\u001b[39;00m idx,finwav \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwavlist):\n",
      "File \u001b[0;32m/idiap/temp/ibmahmoud/miniconda3/envs/s3prl-pytorch/lib/python3.9/enum.py:429\u001b[0m, in \u001b[0;36mEnumMeta.__getattr__\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_member_map_[name]\n\u001b[1;32m    428\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: name"
     ]
    }
   ],
   "source": [
    "smile=smile_features(dataset_train.path)\n",
    "print(callable(smile))\n",
    "smile.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [0,10,20,30,40,50, 70,80, 100, 150, 200],\n",
    "    \"max_depth\": [None, 5, 7 , 10],\n",
    "    \"min_samples_split\": [2, 3,5, 7,10],\n",
    "    \"min_samples_leaf\": [1, 2, 3,4]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test=FEATS_test[:,:-1]\n",
    "labels_test=FEATS_test[:,-1]\n",
    "features_train=FEATS_train[:,:-1]\n",
    "labels_train=FEATS_train[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FEATS_test.shape)\n",
    "print(labels_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(features_train, labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=grid_search.best_estimator_\n",
    "y_pred=best_model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion=metrics.confusion_matrix(labels_test,y_pred)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.diag(confusion) / np.sum(confusion,axis=1))/6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['linear','rbf', 'poly', 'sigmoid']}\n",
    "grid=GridSearchCV(svm.SVC(),param_grid,refit=True,verbose=2)\n",
    "grid.fit(features_train,labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(features_test)\n",
    "confusion_m=metrics.confusion_matrix(labels_test,grid_predictions)\n",
    "confusion_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.diag(confusion_m) / np.sum(confusion_m,axis=1))/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s3prl-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
